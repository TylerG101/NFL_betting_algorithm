{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec9ef979",
   "metadata": {},
   "source": [
    "# Scrape stats from Pro Football Reference\n",
    "## Keep stats as rolling averages for the season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea9425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Now create a function that does all of this for a given team and year\n",
    "import pandas as pd\n",
    "import time\n",
    "import random \n",
    "\n",
    "abbr_list = ['crd', 'atl', 'rav', 'buf', 'car', 'chi', 'cin', 'cle', 'dal', 'den', 'det', 'gnb', 'htx', 'clt', 'jax', 'kan', 'rai',\n",
    "             'sdg', 'ram', 'mia', 'min', 'nwe', 'nor', 'nyg', 'nyj', 'phi', 'pit', 'sea', 'sfo', 'tam', 'oti', 'was']\n",
    "\n",
    "stats = {'Result':'Rslt', 'Location':'Unnamed: 5', 'Week':'Week', 'Pass Yds':'Yds',  'Rush Yds':'Yds.2', 'Points per game':'Pts', 'Sacks':'Sk', 'Cmp':'Cmp%',\n",
    "              'Plays':'Ply', '3DConv':'3DConv', 'Yards per Play':'Y/P'}\n",
    "\n",
    "\n",
    "def create_stat_df(team:str, year:str, main_df:pd.DataFrame):\n",
    "    ###Get the stat table we'll be scraping from\n",
    "    url = 'https://www.pro-football-reference.com/teams/' + team + '/' + year + '/gamelog/'\n",
    "    team_df = pd.read_html(url, header=1, attrs={'id':'table_pfr_team-year_game-logs_team-year-regular-season-game-log'})[0]\n",
    "    opp_df = pd.read_html(url, header=1, attrs={'id':'table_pfr_team-year_game-logs_team-year-regular-season-opponent-game-log'})[0]\n",
    "    ###Loop over the stats getting both team and opp info for each one\n",
    "    print(f'Scraping stats for {team}, {year}')\n",
    "    \n",
    "    teamname_col = [f'{team}'] * (len(team_df) - 1)\n",
    "    teamname_df = pd.DataFrame(teamname_col)\n",
    "    teamname_df.columns = ['Team']\n",
    "    main_df = pd.concat([main_df, teamname_df], axis=1)\n",
    "\n",
    "    for stat in stats:\n",
    "        if stat == 'Location' or stat == 'Week' or stat == \"Result\":\n",
    "            ####Just loop once b/c we dont need opp info\n",
    "            #Get stat\n",
    "            season_tot = []\n",
    "            for i in range(len(team_df)-1):                           \n",
    "                season_tot.append(team_df[stats[stat]][i])\n",
    "            \n",
    "            #Concat to df\n",
    "            tot_df = pd.DataFrame(season_tot)\n",
    "            tot_df.columns = [stat]\n",
    "            main_df = pd.concat([main_df, tot_df], axis=1)\n",
    "\n",
    "\n",
    "        elif stat == \"3DConv\":\n",
    "            third_pg, third_pg_a = [], []\n",
    "            perc, perc_a = 0, 0\n",
    "\n",
    "            for i in range(len(team_df)-1):\n",
    "                perc += team_df['3DConv'][i] / team_df['3DAtt'][i]\n",
    "                avg_third = perc / team_df['Week'][i]\n",
    "                third_pg.append(avg_third.item())\n",
    "            \n",
    "                perc_a += opp_df['3DConv'][i] / opp_df['3DAtt'][i]\n",
    "                avg_third_a = perc_a / opp_df['Week'][i]\n",
    "                third_pg_a.append(avg_third_a.item())\n",
    "            \n",
    "            third_df = pd.DataFrame(third_pg)\n",
    "            third_df.columns = ['3DConv']\n",
    "            third_df_a = pd.DataFrame(third_pg_a)\n",
    "            third_df_a.columns = ['3DConv Allowed']\n",
    "\n",
    "            main_df = pd.concat([main_df, third_df, third_df_a], axis=1)\n",
    "\n",
    "\n",
    "        else:\n",
    "            #print(f'On stat: {stat}')\n",
    "            team_pg, opp_pg = [], []\n",
    "            team, opp = 0,0\n",
    "            for i in range(len(team_df)-1):\n",
    "                #print(f'team value is: {team}')\n",
    "                team += team_df[stats[stat]][i]\n",
    "                avg_team = team / team_df['Week'][i]\n",
    "                team_pg.append(avg_team.item())\n",
    "\n",
    "                #print(f'opp value is: {opp}')\n",
    "                opp += opp_df[stats[stat]][i]\n",
    "                avg_opp = opp / opp_df['Week'][i]\n",
    "                opp_pg.append(avg_opp.item())\n",
    "\n",
    "            cur_team_df = pd.DataFrame(team_pg)\n",
    "            cur_team_df.columns = [stat]\n",
    "            cur_opp_df = pd.DataFrame(opp_pg)\n",
    "            cur_opp_df.columns = [\"Opp \" + stat]\n",
    "\n",
    "            main_df = pd.concat([main_df, cur_team_df, cur_opp_df], axis=1)\n",
    "            \n",
    "\n",
    "    time.sleep(random.randint(8, 10))\n",
    "\n",
    "    return main_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987f2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "681443aa",
   "metadata": {},
   "source": [
    "# Generalize for all teams and all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c0e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr_list = ['crd', 'atl', 'rav', 'buf', 'car', 'chi', 'cin', 'cle', 'dal', 'den', 'det', 'gnb', 'htx', 'clt', 'jax', 'kan', 'rai',\n",
    "             'sdg', 'ram', 'mia', 'min', 'nwe', 'nor', 'nyg', 'nyj', 'phi', 'pit', 'sea', 'sfo', 'tam', 'oti', 'was']\n",
    "\n",
    "#For now we'll scrape one year at a time for the purpose of merging with the weather df later (will update to scrape all years at once)\n",
    "years = [str(season) for season in range(2024, 2025)]\n",
    "\n",
    "overall_stats_df = pd.DataFrame()\n",
    "main_df = pd.DataFrame()\n",
    "\n",
    "for year in years:\n",
    "    for team in abbr_list:\n",
    "        complete_team_df = create_stat_df(team, year, main_df)\n",
    "        overall_stats_df = pd.concat([overall_stats_df, complete_team_df], axis=0)\n",
    "\n",
    "overall_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09603b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8c6bcae",
   "metadata": {},
   "source": [
    "# Scrape weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebb6c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "def scrape_weather(year:str, base_df:pd.DataFrame):\n",
    "    #for year in year:\n",
    "    #We need to account for the switch to an 18 week season in 2021\n",
    "    if int(year) <= 2020:\n",
    "        weeks = [str(week) for week in range(1,18)]\n",
    "    else:\n",
    "        weeks = [str(week) for week in range(1,19)]\n",
    "        \n",
    "    for week in weeks:\n",
    "        url = 'https://www.nflweather.com/week/' + year + '/week-' + week\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise error if request failed\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all game weather rows\n",
    "        games = soup.find_all(class_='mx-2')\n",
    "        #Get correspoinding team names\n",
    "        teams = soup.find_all('span', class_=\"fw-bold\")\n",
    "\n",
    "        #Get team names from html for matchups\n",
    "        team_list = []\n",
    "        for i in range(len(teams) - 1):\n",
    "            text = teams[i].get_text()\n",
    "            if text != '@':\n",
    "                team_list.append(text)\n",
    "        #Create a list of matchups from team_list that correspond to the weather data\n",
    "        matchups = []\n",
    "        n = 0\n",
    "        while n < len(team_list):\n",
    "            vs = team_list[n] + \" \" + team_list[n+1]\n",
    "            matchups.append(vs)\n",
    "            n += 2  \n",
    "        #Get the temperature and weather data from html\n",
    "        m = 0\n",
    "        temp = []\n",
    "        weather = []\n",
    "        while m < len(games):\n",
    "            stripped_temp = games[m]\n",
    "            stripped_weat = games[m+1]\n",
    "            #print(stripped_temp.get_text(), stripped_weat.get_text())\n",
    "            temp.append(games[m].get_text())\n",
    "            weather.append(games[m+1].get_text())\n",
    "            m += 2\n",
    "\n",
    "        Matchup_df = pd.DataFrame(matchups)\n",
    "        temp_df = pd.DataFrame(temp)\n",
    "        weather_df = pd.DataFrame(weather)\n",
    "        combined = pd.concat([Matchup_df, temp_df, weather_df], axis=1, ignore_index=True)\n",
    "        combined.columns = [\"Matchup\", \"Temperature\", \"Weather\"]\n",
    "        combined['Week'] = float(week)\n",
    "\n",
    "        base_df = pd.concat([base_df, combined], axis=0)\n",
    "\n",
    "        time.sleep(random.randint(8, 10))\n",
    "\n",
    "            \n",
    "\n",
    "    return base_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb79610",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Now we can scrape the weather logs for a given year\n",
    "year = str(2024)\n",
    "base_df = pd.DataFrame()\n",
    "weather_2024 = scrape_weather(year=year, base_df=base_df)\n",
    "\n",
    "weather_2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d11bd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_name_conversion = {'Cardinals':'crd', 'Falcons':'atl', 'Ravens':'rav', 'Bills':'buf', 'Panthers':'car', 'Bears':'chi', 'Bengals':'cin', 'Browns':'cle', 'Cowboys':'dal', 'Broncos':'den', 'Lions':'det', 'Packers':'gnb', 'Texans':'htx', 'Colts':'clt', 'Jaguars':'jax', 'Chiefs':'kan', 'Raiders':'rai',\n",
    "             'Chargers':'sdg', 'Rams':'ram', 'Dolphins':'mia', 'Vikings':'min', 'Patriots':'nwe', 'Saints':'nor', 'Giants':'nyg', 'Jets':'nyj', 'Eagles':'phi', 'Steelers':'pit', 'Seahawks':'sea', '49ers':'sfo', 'Buccaneers':'tam', 'Titans':'oti', 'Washington':'was'}\n",
    "\n",
    "def edit_weather_columns(weather_df):\n",
    "    ###This function will replace the team names in the 'Matchup' column with thier representation in the stats_df\n",
    "    two_team_list = weather_df['Matchup']\n",
    "    Matchup_array = []\n",
    "\n",
    "    for i in two_team_list:\n",
    "        fir_name, sec_name = i.split()\n",
    "        for j in team_name_conversion:\n",
    "            if j == fir_name:\n",
    "                first = team_name_conversion[j]\n",
    "            elif j == sec_name:\n",
    "                second = team_name_conversion[j]\n",
    "\n",
    "        matchup = first + \" \" + second\n",
    "        Matchup_array.append(matchup)\n",
    "\n",
    "    fixed_matchup_df = pd.DataFrame(Matchup_array)\n",
    "    fixed_matchup_df.columns = ['Matchup']\n",
    "\n",
    "    #Drop the column with the old team names\n",
    "    dropped = weather_df.drop(['Matchup'], axis=1)\n",
    "    dropped = dropped.reset_index(drop=True)\n",
    "    #dropped\n",
    "\n",
    "\n",
    "    #Concatenate the dataframe with the adjusted team names\n",
    "    Weather_df_fin = pd.concat([fixed_matchup_df, dropped], axis=1, ignore_index=True)\n",
    "    Weather_df_fin = Weather_df_fin.rename(columns={0:'Matchup', 1:'Temperature', 2:'Weather', 3:'Week'})\n",
    "\n",
    "    return Weather_df_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d31c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_2024_weather_df = edit_weather_columns(weather_2024)\n",
    "\n",
    "renamed_2024_weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e39d3",
   "metadata": {},
   "source": [
    "# Combine weather dataframe with statistics dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a600b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Split 'Matchup' and make two new columns, each with one of the team names\n",
    "def merge_stats_weather(stats_df:pd.DataFrame, weather_df:pd.DataFrame):\n",
    "\n",
    "    weather_df[['Team1', 'Team2']] = weather_df['Matchup'].str.split(' ', expand=True)\n",
    "    weather_df = weather_df.drop('Matchup', axis=1)\n",
    "    #weather_df\n",
    "\n",
    "    #We do this so we can join with the stats df on Team1 or Team2 = Team\n",
    "    flipped_teams = weather_df.copy()\n",
    "    flipped_teams[['Team1', 'Team2']] = flipped_teams[['Team2', 'Team1']]\n",
    "\n",
    "    merge_ready_weather_df = pd.DataFrame(pd.concat([weather_df, flipped_teams]))\n",
    "\n",
    "    merged_df = pd.merge(stats_df, merge_ready_weather_df, left_on=['Team', 'Week'], right_on=['Team1', 'Week'], how='inner')\n",
    "\n",
    "    ###Get rid of redundant information\n",
    "    merged_df = merged_df.drop('Team1', axis=1)\n",
    "\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2778e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_2024_df = merge_stats_weather(overall_stats_df, renamed_2024_weather_df)\n",
    "\n",
    "merged_2024_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1852c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_2024_df.to_csv(\"NFL_2024_dataframe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d5bddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f6f243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f17b8137",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212062e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll use the scikit-learn library to encode, standardize, and normalize our data\n",
    "#First, we'll combine the datasets from years 2018-2023 to serve as our training data, with 2024 being the test dataset\n",
    "dataset_2017 = pd.read_csv(\"NFL_2017_dataframe.csv\")\n",
    "dataset_2018 = pd.read_csv(\"NFL_2018_dataframe.csv\")\n",
    "dataset_2019 = pd.read_csv(\"NFL_2019_dataframe.csv\")\n",
    "dataset_2020 = pd.read_csv(\"NFL_2020_dataframe.csv\")\n",
    "dataset_2021 = pd.read_csv(\"NFL_2021_dataframe.csv\")\n",
    "dataset_2022 = pd.read_csv(\"NFL_2022_dataframe.csv\")\n",
    "dataset_2023 = pd.read_csv(\"NFL_2023_dataframe.csv\")\n",
    "\n",
    "train_data_raw = pd.DataFrame(pd.concat([dataset_2017, dataset_2018, dataset_2019, dataset_2020, dataset_2021, dataset_2022, dataset_2023]))\n",
    "train_data_raw = train_data_raw.drop(['Team1', 'Team2'], axis=1)\n",
    "\n",
    "test_data_raw = pd.read_csv(\"NFL_2024_dataframe.csv\")\n",
    "test_data_raw = test_data_raw.drop(['Team1', 'Team2'], axis=1)\n",
    "test_data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbac1981",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Import the necessary processing objects and functions from sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3924f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Start by encoding the target column\n",
    "le = LabelEncoder()\n",
    "label_column = le.fit_transform(train_data_raw['Result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d3e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###The Location column can be binarized for home or away (NaN or @ here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996cd475",
   "metadata": {},
   "outputs": [],
   "source": [
    "###The weather column will be one hot encoded (may use hash encoding instead for memory conservation)\n",
    "ohe = OneHotEncoder()\n",
    "Weather = ohe.fit_transform(pd.DataFrame(train_data_raw['Weather']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fae2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "###The team names will be hash encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d52ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "###The rest of the features are numerical, so we will standardize and normalize (temperature is included here, we just need to strip off the °F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff609e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b1f17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "General_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
